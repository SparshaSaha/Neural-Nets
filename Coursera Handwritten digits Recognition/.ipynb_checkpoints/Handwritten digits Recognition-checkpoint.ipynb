{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "fil=scipy.io.loadmat('ex3data1.mat')\n",
    "y_data=fil.get('y')\n",
    "x_data=fil.get('X')\n",
    "print(len(x_data[0]))\n",
    "x_train=[]\n",
    "x_test=[]\n",
    "\n",
    "for i in range(0,400):\n",
    "    x_train.append(x_data[i])\n",
    "for i in range(400,500):\n",
    "    x_test.append(x_data[i])\n",
    "\n",
    "for i in range(500,900):\n",
    "    x_train.append(x_data[i])\n",
    "for i in range(900,1000):\n",
    "    x_test.append(x_data[i])\n",
    "\n",
    "for i in range(1000,1400):\n",
    "    x_train.append(x_data[i])\n",
    "for i in range(1400,1500):\n",
    "    x_test.append(x_data[i])\n",
    "\n",
    "for i in range(1500,1900):\n",
    "    x_train.append(x_data[i])\n",
    "for i in range(1900,2000):\n",
    "    x_test.append(x_data[i])\n",
    "\n",
    "for i in range(2000,2400):\n",
    "    x_train.append(x_data[i])\n",
    "for i in range(2400,2500):\n",
    "    x_test.append(x_data[i])\n",
    "    \n",
    "for i in range(2500,2900):\n",
    "    x_train.append(x_data[i])\n",
    "for i in range(2900,3000):\n",
    "    x_test.append(x_data[i])\n",
    "\n",
    "for i in range(3000,3400):\n",
    "    x_train.append(x_data[i])\n",
    "for i in range(3400,3500):\n",
    "    x_test.append(x_data[i])\n",
    "\n",
    "for i in range(3500,3900):\n",
    "    x_train.append(x_data[i])\n",
    "for i in range(3900,4000):\n",
    "    x_test.append(x_data[i])\n",
    "\n",
    "for i in range(4000,4400):\n",
    "    x_train.append(x_data[i])\n",
    "for i in range(4400,4500):\n",
    "    x_test.append(x_data[i])\n",
    "    \n",
    "for i in range(4500,4900):\n",
    "    x_train.append(x_data[i])\n",
    "for i in range(4900,5000):\n",
    "    x_test.append(x_data[i])\n",
    "    \n",
    "    \n",
    "    \n",
    "x_data=x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training data_y\n",
    "y_data=[]\n",
    "for i in range(400):\n",
    "    y_data.append([0,0,0,0,0,0,0,0,0,1])\n",
    "for i in range(400):   \n",
    "    y_data.append([1,0,0,0,0,0,0,0,0,0])\n",
    "for i in range(400):\n",
    "    y_data.append([0,1,0,0,0,0,0,0,0,0])\n",
    "for i in range(400):\n",
    "    y_data.append([0,0,1,0,0,0,0,0,0,0])\n",
    "for i in range(400):\n",
    "    y_data.append([0,0,0,1,0,0,0,0,0,0])\n",
    "for i in range(400):\n",
    "    y_data.append([0,0,0,0,1,0,0,0,0,0])\n",
    "for i in range(400):\n",
    "    y_data.append([0,0,0,0,0,1,0,0,0,0])\n",
    "for i in range(400):\n",
    "    y_data.append([0,0,0,0,0,0,1,0,0,0])\n",
    "for i in range(400):\n",
    "    y_data.append([0,0,0,0,0,0,0,1,0,0])\n",
    "for i in range(400):\n",
    "    y_data.append([0,0,0,0,0,0,0,0,1,0])\n",
    "\n",
    "#Test Data y\n",
    "y_data_test=[]\n",
    "for i in range(100):\n",
    "    y_data_test.append([0,0,0,0,0,0,0,0,0,1])\n",
    "for i in range(100):   \n",
    "    y_data_test.append([1,0,0,0,0,0,0,0,0,0])\n",
    "for i in range(100):\n",
    "    y_data_test.append([0,1,0,0,0,0,0,0,0,0])\n",
    "for i in range(100):\n",
    "    y_data_test.append([0,0,1,0,0,0,0,0,0,0])\n",
    "for i in range(100):\n",
    "    y_data_test.append([0,0,0,1,0,0,0,0,0,0])\n",
    "for i in range(100):\n",
    "    y_data_test.append([0,0,0,0,1,0,0,0,0,0])\n",
    "for i in range(100):\n",
    "    y_data_test.append([0,0,0,0,0,1,0,0,0,0])\n",
    "for i in range(100):\n",
    "    y_data_test.append([0,0,0,0,0,0,1,0,0,0])\n",
    "for i in range(100):\n",
    "    y_data_test.append([0,0,0,0,0,0,0,1,0,0])\n",
    "for i in range(100):\n",
    "    y_data_test.append([0,0,0,0,0,0,0,0,1,0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define tensorflow network\n",
    "\n",
    "n_nodes_input_layer=400\n",
    "n_nodes_hl1=500\n",
    "n_nodes_hl2=500\n",
    "n_nodes_hl3=500\n",
    "n_classes=10\n",
    "\n",
    "x=tf.placeholder('float',[None,400])\n",
    "y=tf.placeholder('float')\n",
    "\n",
    "hidden_l1={'weights':tf.Variable(tf.random_normal([n_nodes_input_layer,n_nodes_hl1])),'biases':tf.Variable(tf.random_normal([n_nodes_hl1]))}\n",
    "hidden_l2={'weights':tf.Variable(tf.random_normal([n_nodes_hl1,n_nodes_hl2])),'biases':tf.Variable(tf.random_normal([n_nodes_hl2]))}\n",
    "hidden_l3={'weights':tf.Variable(tf.random_normal([n_nodes_hl2,n_nodes_hl3])),'biases':tf.Variable(tf.random_normal([n_nodes_hl3]))}\n",
    "output_layer={'weights':tf.Variable(tf.random_normal([n_nodes_hl3,n_classes])),'biases':tf.Variable(tf.random_normal([n_classes]))}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_forward(data):\n",
    "    \n",
    "    l1=tf.add(tf.matmul(data,hidden_l1['weights']),hidden_l1['biases'])\n",
    "    l1=tf.nn.relu(l1)\n",
    "    \n",
    "    l2=tf.add(tf.matmul(l1,hidden_l2['weights']),hidden_l2['biases'])\n",
    "    l2=tf.nn.relu(l2)\n",
    "    \n",
    "    l3=tf.add(tf.matmul(l2,hidden_l3['weights']),hidden_l3['biases'])\n",
    "    l3=tf.nn.relu(l3)\n",
    "    \n",
    "    output=tf.add(tf.matmul(l3,output_layer['weights']),output_layer['biases'])\n",
    "   \n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_batch(batch_las,batch_va):\n",
    "    get_x=[]\n",
    "    get_y=[]\n",
    "    \n",
    "    for i in range(batch_las,batch_va):\n",
    "        get_x.append(x_data[i])\n",
    "        get_y.append(y_data[i])\n",
    "    return np.matrix(get_x),np.matrix(get_y)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-7-cb63cfee2420>:3: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/util/tf_should_use.py:118: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "('Epoch ', 0, 'completed ', 65, 'loss: ', 2693617.7084960938)\n",
      "('Epoch ', 1, 'completed ', 65, 'loss: ', 1043749.2974243164)\n",
      "('Epoch ', 2, 'completed ', 65, 'loss: ', 579020.6165456772)\n",
      "('Epoch ', 3, 'completed ', 65, 'loss: ', 372954.91175842285)\n",
      "('Epoch ', 4, 'completed ', 65, 'loss: ', 217470.87803649902)\n",
      "('Epoch ', 5, 'completed ', 65, 'loss: ', 129921.53695297241)\n",
      "('Epoch ', 6, 'completed ', 65, 'loss: ', 88927.72998046875)\n",
      "('Epoch ', 7, 'completed ', 65, 'loss: ', 62357.157068014145)\n",
      "('Epoch ', 8, 'completed ', 65, 'loss: ', 44129.78719174862)\n",
      "('Epoch ', 9, 'completed ', 65, 'loss: ', 32539.25500357151)\n",
      "('Epoch ', 10, 'completed ', 65, 'loss: ', 23517.581818903684)\n",
      "('Epoch ', 11, 'completed ', 65, 'loss: ', 16597.16211812198)\n",
      "('Epoch ', 12, 'completed ', 65, 'loss: ', 12075.474541544914)\n",
      "('Epoch ', 13, 'completed ', 65, 'loss: ', 8471.872679829597)\n",
      "('Epoch ', 14, 'completed ', 65, 'loss: ', 5825.659480464377)\n",
      "('Epoch ', 15, 'completed ', 65, 'loss: ', 3972.6104838252068)\n",
      "('Epoch ', 16, 'completed ', 65, 'loss: ', 2865.9507015943527)\n",
      "('Epoch ', 17, 'completed ', 65, 'loss: ', 1734.6594495773315)\n",
      "('Epoch ', 18, 'completed ', 65, 'loss: ', 1203.0394344329834)\n",
      "('Epoch ', 19, 'completed ', 65, 'loss: ', 887.5277062654495)\n",
      "('Epoch ', 20, 'completed ', 65, 'loss: ', 512.8682342171669)\n",
      "('Epoch ', 21, 'completed ', 65, 'loss: ', 354.2518590427171)\n",
      "('Epoch ', 22, 'completed ', 65, 'loss: ', 230.05828857421875)\n",
      "('Epoch ', 23, 'completed ', 65, 'loss: ', 191.98861753940582)\n",
      "('Epoch ', 24, 'completed ', 65, 'loss: ', 156.38578900508583)\n",
      "('Epoch ', 25, 'completed ', 65, 'loss: ', 124.5115752053041)\n",
      "('Epoch ', 26, 'completed ', 65, 'loss: ', 119.18793058395386)\n",
      "('Epoch ', 27, 'completed ', 65, 'loss: ', 69.53827588784043)\n",
      "('Epoch ', 28, 'completed ', 65, 'loss: ', 5.614316344261169)\n",
      "('Epoch ', 29, 'completed ', 65, 'loss: ', 8.045585632324219)\n",
      "('Epoch ', 30, 'completed ', 65, 'loss: ', 1.9688100766641785)\n",
      "('Epoch ', 31, 'completed ', 65, 'loss: ', 1.0990794407916837e-06)\n",
      "('Epoch ', 32, 'completed ', 65, 'loss: ', 1.0013329756475287e-06)\n",
      "('Epoch ', 33, 'completed ', 65, 'loss: ', 8.988178024083027e-07)\n",
      "('Epoch ', 34, 'completed ', 65, 'loss: ', 8.058385674303281e-07)\n",
      "('Epoch ', 35, 'completed ', 65, 'loss: ', 7.486203230655519e-07)\n",
      "('Epoch ', 36, 'completed ', 65, 'loss: ', 6.937860348443792e-07)\n",
      "('Epoch ', 37, 'completed ', 65, 'loss: ', 6.389516329363687e-07)\n",
      "('Epoch ', 38, 'completed ', 65, 'loss: ', 6.008057766848651e-07)\n",
      "('Epoch ', 39, 'completed ', 65, 'loss: ', 5.674281737810816e-07)\n",
      "('Epoch ', 40, 'completed ', 65, 'loss: ', 5.197457539907191e-07)\n",
      "('Epoch ', 41, 'completed ', 65, 'loss: ', 4.95904487252119e-07)\n",
      "('Epoch ', 42, 'completed ', 65, 'loss: ', 4.672949671657989e-07)\n",
      "('Epoch ', 43, 'completed ', 65, 'loss: ', 4.5060602360535995e-07)\n",
      "('Epoch ', 44, 'completed ', 65, 'loss: ', 4.1961229158005153e-07)\n",
      "('Epoch ', 45, 'completed ', 65, 'loss: ', 4.005392213457526e-07)\n",
      "('Epoch ', 46, 'completed ', 65, 'loss: ', 3.766977840768959e-07)\n",
      "('Epoch ', 47, 'completed ', 65, 'loss: ', 3.6000881209474755e-07)\n",
      "('Epoch ', 48, 'completed ', 65, 'loss: ', 3.480880934603192e-07)\n",
      "('Epoch ', 49, 'completed ', 65, 'loss: ', 3.2424665619146253e-07)\n",
      "('Epoch ', 50, 'completed ', 65, 'loss: ', 3.1232590913532476e-07)\n",
      "('Epoch ', 51, 'completed ', 65, 'loss: ', 2.9802100698361755e-07)\n",
      "('Epoch ', 52, 'completed ', 65, 'loss: ', 2.861002599274798e-07)\n",
      "('Epoch ', 53, 'completed ', 65, 'loss: ', 2.741794844496326e-07)\n",
      "('Epoch ', 54, 'completed ', 65, 'loss: ', 2.574903987806465e-07)\n",
      "('Epoch ', 55, 'completed ', 65, 'loss: ', 2.4795380682007817e-07)\n",
      "('Epoch ', 56, 'completed ', 65, 'loss: ', 2.3603298870966682e-07)\n",
      "('Epoch ', 57, 'completed ', 65, 'loss: ', 2.2649635411653435e-07)\n",
      "('Epoch ', 58, 'completed ', 65, 'loss: ', 2.1695971952340187e-07)\n",
      "('Epoch ', 59, 'completed ', 65, 'loss: ', 2.0503892983469996e-07)\n",
      "('Epoch ', 60, 'completed ', 65, 'loss: ', 2.0027060543270636e-07)\n",
      "('Epoch ', 61, 'completed ', 65, 'loss: ', 1.9073395662871917e-07)\n",
      "('Epoch ', 62, 'completed ', 65, 'loss: ', 1.8596561801587086e-07)\n",
      "('Epoch ', 63, 'completed ', 65, 'loss: ', 1.740447999054595e-07)\n",
      "('Epoch ', 64, 'completed ', 65, 'loss: ', 1.6689232040789648e-07)\n",
      "85.3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "prediction=feed_forward(x)\n",
    "cost=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction,labels=y))\n",
    "optimizer=tf.train.AdamOptimizer().minimize(cost)\n",
    "    \n",
    "epochs=65\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    saver=tf.train.Saver();\n",
    "        \n",
    "    for ep in range(epochs):\n",
    "        batch_var=50\n",
    "        batch_last=0\n",
    "        epoch_loss=0\n",
    "        for t in range(4000/50):\n",
    "            xd,yd=get_next_batch(batch_last,batch_var)\n",
    "            _,c=sess.run([optimizer,cost],feed_dict={x:xd,y:yd})\n",
    "            epoch_loss+=c\n",
    "            batch_last+=50\n",
    "            batch_var+=50\n",
    "        print('Epoch ',ep,'completed ',epochs,'loss: ',epoch_loss)\n",
    "    xx=np.matrix(x_test)\n",
    "            \n",
    "    pred=sess.run(prediction,feed_dict={x:xx})\n",
    "    ace=tf.argmax(pred,axis=1).eval()\n",
    "    actual=tf.argmax(y_data_test,axis=1).eval()\n",
    "    e=tf.subtract(actual,ace).eval()\n",
    "    x=tf.count_nonzero(e).eval()\n",
    "    print((1000-x)*100/float(len(y_data_test)))\n",
    "    save_path=saver.save(sess,\"./hand_written_digits.ckpt\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
